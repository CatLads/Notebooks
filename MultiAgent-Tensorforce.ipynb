{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-d89b7321-32a2-43c7-8544-0332af4c7ddc",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "44181fb7",
        "execution_millis": 0,
        "execution_start": 1617181429320,
        "deepnote_cell_type": "code"
      },
      "source": "#!pip install flatland-rl\n#!pip install tensorforce",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-d2359fc2-01d8-45e4-a29c-b3f1b70c5031",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b94fb84e",
        "execution_millis": 2958,
        "execution_start": 1617181429321,
        "deepnote_cell_type": "code"
      },
      "source": "from tensorforce import Agent, Environment\nfrom obs_utils import normalize_observation",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-946ddc55-41ca-41e1-adb3-c83bdb76f7f5",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8301200e",
        "execution_millis": 0,
        "execution_start": 1617185415396,
        "deepnote_cell_type": "code"
      },
      "source": "from flatland.envs.rail_env import RailEnv, RailEnvActions\nimport numpy as np\n\n\nclass OurEnv(RailEnv):\n    def reset(self, *args, render=True, **kwargs):\n        observation, info_dict = super().reset(*args, **kwargs)\n       #self.env_renderer = RenderTool(env)\n        if render:\n            self.step({0: RailEnvActions.MOVE_FORWARD})\n            render_env(self)\n        return observation\n\nclass TensorforceEnv(Environment):\n    def __init__(self, rail_env, num_agents, state_shape):\n        self._rail_env = rail_env\n        self.num_agents = num_agents\n        state, _ = self._rail_env.reset()\n        self.state_shape = state_shape\n        super().__init__()\n    \n    def process_state(self, state):\n        return state #np.array(state[0][1]).flatten()\n\n    def process_reward(self, reward):\n        reward_sum = 0\n        for _, train_reward in reward.items():\n            reward_sum += train_reward\n        return reward_sum\n\n    def states(self):\n        #state = self._rail_env._get_observations()\n        #return dict(type='float', shape=(width,height,16))\n        #return self._state\n        return dict(type=\"float\", min_value=-1000.0, max_value=1000.0, shape=self.state_shape)\n\n    def actions(self):\n        return dict(type='int', num_values=len(RailEnvActions), shape=(1,))\n\n    # Optional additional steps to close environment\n    def close(self):\n        # Maybe render?\n        super().close()\n\n    def reset(self):\n        state, info = self._rail_env.reset()\n        self._state = state\n        #state = np.random.random(size=(8,))\n        return state\n\n    def execute(self, actions):\n        #actions = {index: RailEnvActions(value) for index, value in enumerate(actions)}\n        \n        state, reward, done, info  = self._rail_env.step(actions)\n        \n        if not done[\"__all__\"]:\n            try:\n                state = self.process_state(state)\n            except:\n                print(state)\n            self._state = state\n        reward = self.process_reward(reward)\n        \n        return self._state, done, reward ",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-a27b1519-0744-4f2c-b88b-366c1e6b4c1c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "97b3f3d7",
        "execution_millis": 1,
        "execution_start": 1617182457585,
        "deepnote_cell_type": "code"
      },
      "source": "from flatland.utils.rendertools import RenderTool\nimport matplotlib.pyplot as plt\n\ndef render_env(env, figsize=(8, 8)):\n  \"\"\"Show the environment using matplotlib\"\"\"\n  env_renderer = RenderTool(env, gl=\"PILSVG\")\n  # img is a numpy array\n  img = env_renderer.render_env(show=True, return_image=True)\n\n  plt.figure(figsize=figsize)\n  plt.imshow(img)\n  plt.show()\n",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-e546fa99-ac1b-4667-b7e8-52ba696095f7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "55dd592e",
        "execution_millis": 3251,
        "execution_start": 1617182867005,
        "deepnote_cell_type": "code"
      },
      "source": "from flatland.envs.rail_generators import sparse_rail_generator\nfrom flatland.envs.schedule_generators import sparse_schedule_generator\nfrom flatland.envs.rail_env import RailEnv, RailEnvActions\nfrom flatland.envs.rail_generators import complex_rail_generator\nfrom flatland.envs.observations import GlobalObsForRailEnv, TreeObsForRailEnv\n\nseed = 69 #nice \n\n\nwidth = 10 # @param{type: \"integer\"}\nheight = 10 # @param{type: \"integer\"}\nnum_agents =  2  # @param{type: \"integer\"}\ntree_depth = 2 # @param{type: \"integer\"}\nradius_observation = 10\nWINDOW_LENGTH =   22# @param{type: \"integer\"}\n\n\nrandom_rail_generator = complex_rail_generator(\n    nr_start_goal=10, # @param{type:\"integer\"} number of start and end goals \n                      # connections, the higher the easier it should be for \n                      # the trains\n    nr_extra=10, # @param{type:\"integer\"} extra connections \n                 # (useful for alternite paths), the higher the easier\n    min_dist=10,\n    max_dist=99999,\n    seed=seed\n)\n\n\nenv = RailEnv(\n    width=width,\n    height=height,\n    rail_generator=random_rail_generator,\n    obs_builder_object=TreeObsForRailEnv(tree_depth),\n    number_of_agents=num_agents\n)\n\nobs, info = env.reset()\n\nstate_shape = normalize_observation(obs[0], tree_depth, radius_observation).shape\n\nenvironment = TensorforceEnv(env, num_agents, state_shape)",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n#normalize_observation(obs[0], tree_depth, 1000).shape\nobs",
      "metadata": {
        "tags": [],
        "cell_id": "00005-2d7a0f11-4147-46a4-b036-797b16781907",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "fa15a3dc",
        "execution_millis": 1479,
        "execution_start": 1617181457102,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "{0: Node(dist_own_target_encountered=0, dist_other_target_encountered=0, dist_other_agent_encountered=0, dist_potential_conflict=0, dist_unusable_switch=0, dist_to_next_branch=0, dist_min_to_target=41.0, num_agents_same_direction=0, num_agents_opposite_direction=0, num_agents_malfunctioning=0, speed_min_fractional=1.0, num_agents_ready_to_depart=0, childs={'L': -inf, 'F': Node(dist_own_target_encountered=inf, dist_other_target_encountered=inf, dist_other_agent_encountered=inf, dist_potential_conflict=inf, dist_unusable_switch=inf, dist_to_next_branch=6, dist_min_to_target=35.0, num_agents_same_direction=0, num_agents_opposite_direction=0, num_agents_malfunctioning=0, speed_min_fractional=1.0, num_agents_ready_to_depart=0, childs={'L': -inf, 'F': Node(dist_own_target_encountered=inf, dist_other_target_encountered=inf, dist_other_agent_encountered=inf, dist_potential_conflict=inf, dist_unusable_switch=14, dist_to_next_branch=15, dist_min_to_target=26.0, num_agents_same_direction=0, num_agents_opposite_direction=0, num_agents_malfunctioning=0, speed_min_fractional=1.0, num_agents_ready_to_depart=0, childs={}), 'R': -inf, 'B': -inf}), 'R': -inf, 'B': -inf}),\n 1: Node(dist_own_target_encountered=0, dist_other_target_encountered=0, dist_other_agent_encountered=0, dist_potential_conflict=0, dist_unusable_switch=0, dist_to_next_branch=0, dist_min_to_target=14.0, num_agents_same_direction=0, num_agents_opposite_direction=0, num_agents_malfunctioning=0, speed_min_fractional=1.0, num_agents_ready_to_depart=0, childs={'L': -inf, 'F': Node(dist_own_target_encountered=inf, dist_other_target_encountered=inf, dist_other_agent_encountered=inf, dist_potential_conflict=inf, dist_unusable_switch=2, dist_to_next_branch=4, dist_min_to_target=10.0, num_agents_same_direction=0, num_agents_opposite_direction=0, num_agents_malfunctioning=0, speed_min_fractional=1.0, num_agents_ready_to_depart=0, childs={'L': -inf, 'F': Node(dist_own_target_encountered=inf, dist_other_target_encountered=inf, dist_other_agent_encountered=inf, dist_potential_conflict=inf, dist_unusable_switch=inf, dist_to_next_branch=6, dist_min_to_target=8.0, num_agents_same_direction=0, num_agents_opposite_direction=0, num_agents_malfunctioning=0, speed_min_fractional=1.0, num_agents_ready_to_depart=0, childs={}), 'R': -inf, 'B': -inf}), 'R': -inf, 'B': -inf})}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00009-253a3dfa-0199-422a-89b5-472b82079c1c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a2b9e9de",
        "execution_millis": 334,
        "execution_start": 1617182870259,
        "deepnote_cell_type": "code"
      },
      "source": "from tensorforce.agents import DeepQNetwork\n\nagent = DeepQNetwork.create(\n    agent='tensorforce',\n    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n    memory=10000,\n    update=dict(unit='timesteps', batch_size=64),\n    optimizer=dict(type='adam', learning_rate=3e-4),\n    policy=dict(network='auto'),\n    objective='policy_gradient',\n    reward_estimation=dict(horizon=20)\n)",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "'''         # Reset environment\n        obs, info = env.reset(True, True)\n        env_renderer.reset()\n        # Build agent specific observations\n        for a in range(env.get_num_agents()):\n            if obs[a]:\n                agent_obs[a] = normalize_observation(obs[a], tree_depth, observation_radius=10)\n                agent_obs_buffer[a] = agent_obs[a].copy()\n\n        # Reset score and done\n        score = 0\n        env_done = 0\n\n\n\nfor agent in range(env.get_num_agents()):\n                # Only update the values when we are done or when an action was taken and thus relevant information is present\n                if update_values or done[agent]:\n                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n\n                    agent_prev_obs[agent] = agent_obs[agent].copy()\n                    agent_prev_action[agent] = action_dict[agent]\n\n                if next_obs[agent]:\n                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=10)\n\n                score += all_rewards[agent]\n\n            if done['__all__']:\n                break\n '''",
      "metadata": {
        "tags": [],
        "cell_id": "00006-f8324ec0-449a-42ea-ba45-9f92ec9bab5f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e8d92ce0",
        "execution_start": 1617181458984,
        "execution_millis": 98,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "\"         # Reset environment\\n        obs, info = env.reset(True, True)\\n        env_renderer.reset()\\n        # Build agent specific observations\\n        for a in range(env.get_num_agents()):\\n            if obs[a]:\\n                agent_obs[a] = normalize_observation(obs[a], tree_depth, observation_radius=10)\\n                agent_obs_buffer[a] = agent_obs[a].copy()\\n\\n        # Reset score and done\\n        score = 0\\n        env_done = 0\\n\\n\\n\\nfor agent in range(env.get_num_agents()):\\n                # Only update the values when we are done or when an action was taken and thus relevant information is present\\n                if update_values or done[agent]:\\n                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\\n\\n                    agent_prev_obs[agent] = agent_obs[agent].copy()\\n                    agent_prev_action[agent] = action_dict[agent]\\n\\n                if next_obs[agent]:\\n                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=10)\\n\\n                score += all_rewards[agent]\\n\\n            if done['__all__']:\\n                break\\n \""
          },
          "metadata": {}
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-99e7d575-c4e5-49d7-88de-49c89dd82879",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b5c05f2e",
        "execution_millis": 3166,
        "output_cleared": false,
        "execution_start": 1617185467297,
        "deepnote_cell_type": "code"
      },
      "source": "# Train for 300 episodes\nfor episode in range(300):\n\n    episode_states = list()\n    episode_internals = list()\n    episode_actions = list()\n    episode_terminal = list()\n    episode_reward = list()\n\n    # Initialize episode\n    states = environment.reset()\n    internals = agent.initial_internals()\n\n    terminal = {i: False for i in range(0, num_agents)}\n    terminal[\"__all__\"]= False\n    #num_updates = 0\n    sum_rewards = 0.0\n    \n\n    while not terminal[\"__all__\"]:\n        actions = {}\n        agents_obs = {}\n        \n        episode_states.append(states)\n        episode_internals.append(internals)\n\n        for i in range(0, num_agents):\n            if not terminal[i]:\n                agents_obs[i] = normalize_observation(states[i], tree_depth, radius_observation)\n                actions[i], internals = agent.act(agents_obs[i], internals=internals, independent=True)\n                \n        episode_actions.append(actions)\n        states, terminal, reward = environment.execute(actions=actions)\n        \n        episode_terminal.append(terminal[\"__all__\"])\n        episode_reward.append(reward)\n        sum_rewards += reward\n\n    print(episode_terminal)\n    print('Episode {}: {}'.format(episode, sum_rewards))\n    \n    agent.experience(\n        states=episode_states, internals=episode_internals, actions=episode_actions,\n        terminal=episode_terminal, reward=episode_reward\n    )\n\n    # Perform update\n    agent.update()\n        \nagent.close()\nenvironment.close()",
      "execution_count": 101,
      "outputs": [
        {
          "name": "stdout",
          "text": "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\nEpisode 0: -300.0\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "TensorforceError",
          "evalue": "Calling agent.experience is not possible mid-episode.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTensorforceError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-96d7ff2269ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     agent.experience(\n\u001b[1;32m     43\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_internals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mterminal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_terminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorforce/agents/tensorforce.py\u001b[0m in \u001b[0;36mexperience\u001b[0;34m(self, states, actions, terminal, reward, internals)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTensorforceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Calling agent.experience is not possible mid-episode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# Process states input and infer batching structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTensorforceError\u001b[0m: Calling agent.experience is not possible mid-episode."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-4a8834f8-1a25-4a2c-8875-a971e33878f1",
        "deepnote_to_be_reexecuted": true,
        "source_hash": "918ce417",
        "execution_millis": 4290,
        "deepnote_cell_type": "code"
      },
      "source": "from tensorforce.execution import Runner\n\nrunner = Runner(agent=agent, environment=environment, max_episode_timesteps=500)\n\nrunner.run(num_episodes=5)\n\nrunner.run(num_episodes=2, evaluation=True)\n\nrunner.close()",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Episodes:   0%|          | 0/5 [00:00, reward=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb20f556570641b1b26f2fe9b60594aa"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "Episodes:   0%|          | 0/2 [00:00, reward=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ff00f78875b49efa9f659bd84e5891d"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "if not all(len(buffer) == 0 for buffer in self.terminal_buffer):\n            raise TensorforceError(message=\"Calling agent.experience is not possible mid-episode.\")",
      "metadata": {
        "tags": [],
        "cell_id": "00010-87ea9567-681c-4943-9500-ac8d65ab166d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c8b2a743-4403-48d9-b1f8-a1215902878c' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "c7021a44-95ce-44ad-a920-8a044ccf0e98",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}