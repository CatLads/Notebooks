# -*- coding: utf-8 -*-
"""BasicRL.ipynb 

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/FatLads/Notebooks/blob/main/Jip_test_keras.ipynb
"""



"""# Required libraries


* `flatland-rl` - http://flatland-rl-docs.s3-website.eu-central-1.amazonaws.com/
* `keras-rl` - https://github.com/keras-rl/keras-rl
"""


from flatland.envs.rail_generators import sparse_rail_generator
from flatland.envs.schedule_generators import sparse_schedule_generator
from flatland.envs.rail_env import RailEnv, RailEnvActions

"""# Environment

Let's build a simple environment using [GlobalObservation](https://flatland.aicrowd.com/getting-started/env/observations.html#global-observation), the easiest one.
"""

class OurEnv(RailEnv):
    def reset(self, *args, **kwargs):
        return_val = super().reset(*args, **kwargs)
        self.env_renderer = RenderTool(env)
        self.step({0: RailEnvActions.MOVE_FORWARD})
        return return_val
    def step(self, *args, **kwargs):
        self.env_renderer.render_env(show=True)
        print(args[0])
        observation, reward, done, info = super().step(*args, **kwargs)
        return observation, reward, done["__all__"], info

from flatland.envs.rail_env import RailEnv
from flatland.envs.rail_generators import complex_rail_generator
from flatland.envs.observations import GlobalObsForRailEnv

seed = 69 #nice

width = 10 # @param{type: "integer"}
height = 10 # @param{type: "integer"}
agents =  1 # @param{type: "integer"}
WINDOW_LENGTH =   21# @param{type: "integer"}


random_rail_generator = complex_rail_generator(
    nr_start_goal=10, # @param{type:"integer"} number of start and end goals 
                      # connections, the higher the easier it should be for 
                      # the trains
    nr_extra=10, # @param{type:"integer"} extra connections 
                 # (useful for alternite paths), the higher the easier
    min_dist=10,
    max_dist=99999,
    seed=seed
)
from flatland.utils.rendertools import RenderTool

env = OurEnv(
    width=width,
    height=height,
    rail_generator=random_rail_generator,
    obs_builder_object=GlobalObsForRailEnv(),
    number_of_agents=agents
)
env_renderer = RenderTool(env)

# env.reset is needed to build the first step of the env
_ = env.reset() # assigned to _ just to suppress the output

"""To render the env we use RenderTool. I think `gl="PILSVG"` is the lib used to actually render, using the default one doesn't work.

The function `render_env(env)` shows the env using matplotlib.
"""

import matplotlib.pyplot as plt


"""Let's perform a basic action for each train.

The actions are (as defined [here](http://flatland-rl-docs.s3-website.eu-central-1.amazonaws.com/04_specifications.html#action-space)):
* 0 Do Nothing: If the agent is moving it continues moving, if it is stopped it stays stopped
* 1 Deviate Left: If the agent is at a switch with a transition to its left, the agent will chose th eleft path. Otherwise the action has no effect. If the agent is stopped, this action will start agent movement again if allowed by the transitions.
* 2 Go Forward: This action will start the agent when stopped. This will move the agent forward and chose the go straight direction at switches.
* 3 Deviate Right: Exactly the same as deviate left but for right turns.
* 4 Stop: This action causes the agent to stop.

"""



"""Building an env is quite easy and straightforward.

# Training

Loading (and owning) some libs
"""

from PIL import Image
import numpy as np
import gym

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, LSTM, Embedding, Input
from tensorflow.keras.optimizers import Adam
import tensorflow.keras.backend as K

from rl.agents.dqn import DQNAgent
from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy
from rl.memory import SequentialMemory
from rl.core import Processor
from rl.callbacks import FileLogger, ModelIntervalCheckpoint

"""Using Railprocessor and related from simmy"""



class RailProcessor(Processor):
    def process_observation(self, observation):
        if type(observation) is not tuple:
            return np.array(observation[0][1]).flatten()
        return np.array(observation[0][0][1]).flatten() #reshape((5,10,10))[0]
        #return np.array(observation[0][0][0]).flatten() # For now, we'll just keep the transition maps
    def process_reward(self, reward):
        reward_sum = 0
        for _, train_reward in reward.items():
            reward_sum += train_reward
        return reward_sum
    def process_action(self, action):        
        return {0: RailEnvActions(action)}
    def process_step(self, observation, reward, done, info):
        if not done:
            observation = self.process_observation(observation)
        reward = self.process_reward(reward)
        return observation, reward, done, {}

nb_actions = env.action_space[0]
nb_actions

og_input_shape = np.array(env.step({0: RailEnvActions.DO_NOTHING})[0][0][1]).flatten().shape
og_input_shape

"""Build a model, does annoying error which I was not able to fix yet"""

RailEnvActions(4)

print(np.array(env.reset()[0][0][1]).flatten().shape)
print(np.array(env.step({0: RailEnvActions.STOP_MOVING})[0][0][1]).flatten().shape)
print(np.array(env.step({0: RailEnvActions.MOVE_FORWARD})[0][0][1]).flatten().shape)


input_shape = (WINDOW_LENGTH,) + og_input_shape
model = Sequential()
timesteps = 1 #1 timestep as tree is only 1 deep
model.add(Input(shape=input_shape, name="INSERIMENTO_DATI"))
#model.add(Embedding(input_dim=input_shape[0], output_dim=64))
model.add(LSTM(75, return_sequences=True, activation="relu", batch_input_shape=( 4, 5, 500)))
model.add(LSTM(50, return_sequences=True, activation="relu"))
model.add(Flatten())
model.add(Dense(20, activation="relu"))
model.add(Dense(nb_actions, activation="linear"))
print(model.summary())

memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)
processor = RailProcessor()
policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,
                              nb_steps=1000000)
dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,
               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,
               train_interval=10, delta_clip=1.)
dqn.compile(Adam(lr=.00025), metrics=['mae'])


# Okay, now it's time to learn something! We capture the interrupt exception so that training
# can be prematurely aborted. Notice that now you can use the built-in tensorflow.keras callbacks!
weights_filename = 'dqn_flatland_weights.h5f'
checkpoint_weights_filename = 'dqn_flatland_weights_{step}.h5f'
log_filename = 'dqn_flatland_log.json'
callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]
callbacks += [FileLogger(log_filename, interval=100)]
dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)

# After training is done, we save the final weights one more time.
dqn.save_weights(weights_filename, overwrite=True)

# Finally, evaluate our algorithm for 10 episodes.
dqn.test(env, nb_episodes=10, visualize=False)


"""## Random client
from [here](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/training_example.py).
"""

"""
# Initialize the agent, state size is width * height using GlobalObservation
# actions are 5
agent = RandomAgent(width * height, 5)

n_trials = 2 # @param{type:"integer"}
steps_per_episode = 500 # @param{type:"integer"}
for trials in range(1, n_trials + 1):
    obs, info = env.reset()
    
    score = 0
    # Run episode
    for step in range(steps_per_episode):
        # Chose an action for each agent in the environment
        for a in range(env.get_num_agents()):
            action = agent.act(obs[a])
            action_dict.update({a: action})
        
        # Environment step which returns the observations for all agents, 
        # their corresponding reward and whether their are done
        next_obs, all_rewards, done, _ = env.step(action_dict)
        render_env(env)

        if done['__all__']:
            break
    
    print('Episode Nr. {}\t Score = {}'.format(trials, score))
    """
