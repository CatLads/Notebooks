{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDDQN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "deepnote_notebook_id": "c31c442b-ef39-4b97-95f1-2477b566b21c",
    "deepnote": {},
    "deepnote_execution_queue": [
      {
        "cellId": "00006-367ebf90-92ca-4d7d-ac12-7c409e317dbd",
        "sessionId": "f8dc6d81-52cd-44c7-acfe-5dbf33302d5f",
        "msgId": "41b96a76-d263-4186-95da-0351518c50b4"
      }
    ]
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CatLads/Notebooks/blob/main/DDDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-e9791b8d-f664-498d-85cd-f125c351220d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "fb84626b",
        "execution_millis": 5706,
        "output_cleared": false,
        "execution_start": 1618216428647,
        "deepnote_cell_type": "code",
        "id": "zOE3ugStOwHP",
        "outputId": "44a9ec72-9310-4bec-cc13-46ba7ab77ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install flatland-rl tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flatland-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/30/e002f8b7d9075c88f2f00e870294e7896c92db8b3c94ae9c442ca0e42bc2/flatland-rl-2.2.2.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 18.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting tox>=3.5.2\n",
            "  Using cached https://files.pythonhosted.org/packages/ec/7e/4609fd0386d41f0b94fe952708970fb87cc1fb66e088758b1f0ab336802e/tox-3.23.0-py2.py3-none-any.whl\n",
            "Collecting pytest<5,>=3.8.2\n",
            "  Using cached https://files.pythonhosted.org/packages/70/c7/e8cb4a537ee4fc497ac80a606a667fd1832f28ad3ddbfa25bf30473eae13/pytest-4.6.11-py2.py3-none-any.whl\n",
            "Collecting pytest-runner>=4.2\n",
            "  Using cached https://files.pythonhosted.org/packages/40/96/9024a1c07bbe5e16bdcbcbd021b608e37b32df4301ae2090aad27c24ffe6/pytest_runner-5.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (7.1.2)\n",
            "Collecting crowdai-api>=0.1.21\n",
            "  Using cached https://files.pythonhosted.org/packages/0c/ee/55912b05af8994a190280e3281a18720f8d69da02dcb7ff44e1b96974345/crowdai_api-0.1.22.tar.gz\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (1.19.5)\n",
            "Collecting recordtype>=1.3\n",
            "  Using cached https://files.pythonhosted.org/packages/60/9a/835ba329e31aa471a5597c733f7ca0136b3a0622ce01b9e66b40f5909da4/recordtype-1.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (7.1.2)\n",
            "Collecting msgpack==0.6.1\n",
            "  Using cached https://files.pythonhosted.org/packages/a8/7b/630049fc4af9e68a625738612edc264ce7cb586c5001a2d4d2209a4f61c1/msgpack-0.6.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting msgpack-numpy>=0.4.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl\n",
            "Collecting svgutils>=0.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/44/79/0367ebd8a2edfdc46332b90bce1fd183e25078ed1b0d446c6bf42ea7ba7a/svgutils-0.3.4-py3-none-any.whl\n",
            "Requirement already satisfied: pyarrow>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (3.0.0)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.17 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (3.8.1)\n",
            "Collecting importlib-resources<2,>=1.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/7f/2d/88f166bcaadc09d9fdbf1c336ad118e01b7fe1155e15675e125be2ff1899/importlib_resources-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (1.15.0)\n",
            "Collecting timeout-decorator>=0.4.1\n",
            "  Using cached https://files.pythonhosted.org/packages/80/f8/0802dd14c58b5d3d72bb9caa4315535f58787a1dc50b81bbbcaaa15451be/timeout-decorator-0.5.0.tar.gz\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (20.3.0)\n",
            "Collecting gym==0.14.0\n",
            "  Using cached https://files.pythonhosted.org/packages/61/75/9e841bc2bc75128e0b65c3d5255d0bd16becb9d8f7120b965d41b8e70041/gym-0.14.0.tar.gz\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (2.5)\n",
            "Collecting ipycanvas\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/66/cf77aaa148324f8d4eb05fb75899701c670ca98bd2c96862f569d3d0c0a5/ipycanvas-0.8.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (0.10.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from flatland-rl) (2.4.1)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from tox>=3.5.2->flatland-rl) (0.10.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from tox>=3.5.2->flatland-rl) (3.0.12)\n",
            "Requirement already satisfied: packaging>=14 in /usr/local/lib/python3.7/dist-packages (from tox>=3.5.2->flatland-rl) (20.9)\n",
            "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/91/fb/ca6c071f4231e06a9f0c3bd81c15c233bbacd4a7d9dbb7438d95fece8a1e/virtualenv-20.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.7/dist-packages (from tox>=3.5.2->flatland-rl) (1.10.0)\n",
            "Collecting pluggy>=0.12.0\n",
            "  Using cached https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (8.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (1.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (0.2.5)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.23.0)\n",
            "Collecting python-gitlab>=1.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/91/4a/4b3f68268499f4ffa51921fbacec3f25b5df842c9a4be3ec877be94a2746/python_gitlab-2.6.0-py3-none-any.whl\n",
            "Collecting redis\n",
            "  Using cached https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.4.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from svgutils>=0.3.1->flatland-rl) (4.2.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->flatland-rl) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.17->flatland-rl) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.17->flatland-rl) (3.7.4.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.14.0->flatland-rl) (1.4.1)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
            "Collecting cloudpickle~=1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->flatland-rl) (4.4.2)\n",
            "Collecting orjson\n",
            "  Using cached https://files.pythonhosted.org/packages/39/c3/56c4732c1d43e2d72749ea1acab7288302972e29fb3a778fdb7b804053f8/orjson-3.5.1-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.7/dist-packages (from ipycanvas->flatland-rl) (7.6.3)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/0a/490fa011d699bb5a5f3a0cf57de82237f52a6db9d40f33c53b2736c9a1f9/distlib-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: appdirs<2,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox>=3.5.2->flatland-rl) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (3.0.4)\n",
            "Collecting requests-toolbelt>=0.9.1\n",
            "  Using cached https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.14.0->flatland-rl) (0.16.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.1.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.0.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.0.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.1.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.3.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (54.2.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (22.0.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.9.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (5.6.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.1.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipycanvas->flatland-rl) (0.5.1)\n",
            "Building wheels for collected packages: flatland-rl, crowdai-api, timeout-decorator, gym\n",
            "  Building wheel for flatland-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flatland-rl: filename=flatland_rl-2.2.2-py2.py3-none-any.whl size=1690840 sha256=87e4af5f5a28566fa626b9c78c9f734fee01318da827767430143de0ce031612\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/c0/b9/153b0e6135cd9a01d550b5e5735fb1c19fbde1ac39c83e18c0\n",
            "  Building wheel for crowdai-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crowdai-api: filename=crowdai_api-0.1.22-py2.py3-none-any.whl size=10000 sha256=aa6ef36fd41a2e243171d026aa3d6132288bcd0d4d31933dcfe4895495a756f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/a2/87/fc28f0db513219afc295b27e829a1f2d74cf2886d0f03aa4ef\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-cp37-none-any.whl size=5031 sha256=dcbe5020274cf8f783e86de9b4516b166c0f729493393caf5e30c3a3f493257f\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/ea/e7/64dd533737dd2b14a718a061e3a0e0baa2ce0ad50b519514ea\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.14.0-cp37-none-any.whl size=1637526 sha256=d9632d3276b3b4d1b077517170f9315314b5d5bfbabf2d7c6f4ae282d578c545\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/53/f6/c0cd3c9bf953f35c0aee7fa62ea209371e92f5e5cced3245ba\n",
            "Successfully built flatland-rl crowdai-api timeout-decorator gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: distlib, virtualenv, pluggy, tox, pytest, pytest-runner, requests-toolbelt, python-gitlab, redis, crowdai-api, recordtype, msgpack, msgpack-numpy, svgutils, importlib-resources, timeout-decorator, pyglet, cloudpickle, gym, orjson, ipycanvas, flatland-rl\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: msgpack 1.0.2\n",
            "    Uninstalling msgpack-1.0.2:\n",
            "      Successfully uninstalled msgpack-1.0.2\n",
            "  Found existing installation: importlib-resources 5.1.2\n",
            "    Uninstalling importlib-resources-5.1.2:\n",
            "      Successfully uninstalled importlib-resources-5.1.2\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed cloudpickle-1.2.2 crowdai-api-0.1.22 distlib-0.3.1 flatland-rl-2.2.2 gym-0.14.0 importlib-resources-1.5.0 ipycanvas-0.8.2 msgpack-0.6.1 msgpack-numpy-0.4.7.1 orjson-3.5.1 pluggy-0.13.1 pyglet-1.3.2 pytest-4.6.11 pytest-runner-5.3.0 python-gitlab-2.6.0 recordtype-1.3 redis-3.5.3 requests-toolbelt-0.9.1 svgutils-0.3.4 timeout-decorator-0.5.0 tox-3.23.0 virtualenv-20.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-019067b7-38fe-48b9-915a-920c00ced50e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "62b710a7",
        "execution_millis": 2564,
        "execution_start": 1618216434364,
        "deepnote_cell_type": "code",
        "id": "fzzEb-E1OwHU"
      },
      "source": [
        "from flatland.envs.rail_generators import sparse_rail_generator\n",
        "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
        "from flatland.envs.rail_env import RailEnv, RailEnvActions\n",
        "from flatland.envs.rail_generators import complex_rail_generator\n",
        "from flatland.envs.observations import GlobalObsForRailEnv, TreeObsForRailEnv\n",
        "from obs_utils import normalize_observation\n",
        "from gym import spaces\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "seed = 69 #nice \n",
        "\n",
        "\n",
        "width = 10 # @param{type: \"integer\"}\n",
        "height = 10 # @param{type: \"integer\"}\n",
        "num_agents =  2  # @param{type: \"integer\"}\n",
        "tree_depth = 2 # @param{type: \"integer\"}\n",
        "radius_observation = 10\n",
        "WINDOW_LENGTH =   22# @param{type: \"integer\"}\n",
        "\n",
        "\n",
        "random_rail_generator = complex_rail_generator(\n",
        "    nr_start_goal=10, # @param{type:\"integer\"} number of start and end goals \n",
        "                      # connections, the higher the easier it should be for \n",
        "                      # the trains\n",
        "    nr_extra=10, # @param{type:\"integer\"} extra connections \n",
        "                 # (useful for alternite paths), the higher the easier\n",
        "    min_dist=10,\n",
        "    max_dist=99999,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "env = RailEnv(\n",
        "    width=width,\n",
        "    height=height,\n",
        "    rail_generator=random_rail_generator,\n",
        "    obs_builder_object=TreeObsForRailEnv(tree_depth),\n",
        "    number_of_agents=num_agents\n",
        ")\n",
        "\n",
        "obs, info = env.reset()\n",
        "\n",
        "\n",
        "state_shape = normalize_observation(obs[0], tree_depth, radius_observation).shape\n",
        "action_shape = (5,)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMCXZFsn2ldc",
        "cell_id": "00001-75aee770-84a2-4fc3-9bba-6088540fb96e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1c3bcded",
        "execution_millis": 4620,
        "execution_start": 1618216436932,
        "deepnote_cell_type": "code"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x905pF9e20y6",
        "cell_id": "00004-c8981471-911c-43f4-aea9-42cfc91853ad",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ff644ea8",
        "execution_millis": 21,
        "execution_start": 1618216441557,
        "deepnote_cell_type": "code"
      },
      "source": [
        "class DDDQN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "      super(DDDQN, self).__init__()\n",
        "      self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "      self.d2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "      self.v = tf.keras.layers.Dense(1, activation=None)\n",
        "      self.a = tf.keras.layers.Dense(5, activation=None) # TODO: Change this please\n",
        "\n",
        "    def call(self, input_data):\n",
        "      x = self.d1(input_data)\n",
        "      x = self.d2(x)\n",
        "      v = self.v(x)\n",
        "      a = self.a(x)\n",
        "      Q = v +(a -tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
        "      return Q\n",
        "\n",
        "    def advantage(self, state):\n",
        "      x = self.d1(state)\n",
        "      x = self.d2(x)\n",
        "      a = self.a(x)\n",
        "      return a\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF7gFMGU8Zk8",
        "cell_id": "00005-7758ff4e-30bc-407f-8cd2-00b9685e2633",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "480b5dca",
        "execution_millis": 50,
        "execution_start": 1618216441607,
        "deepnote_cell_type": "code"
      },
      "source": [
        "class exp_replay():\n",
        "    def __init__(self, buffer_size= 1000000):\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "        self.state_mem = np.zeros((self.buffer_size, *state_shape), dtype=np.float32)\n",
        "        self.action_mem = np.zeros((self.buffer_size), dtype=np.int32) #SIMMY: I removed , *action_shape cause we should just save one action per agent, not all of them\n",
        "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
        "        self.next_state_mem = np.zeros((self.buffer_size, *state_shape), dtype=np.float32)\n",
        "        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool)\n",
        "        self.pointer = 0\n",
        "\n",
        "    def add_exp(self, state, action, reward, next_state, done):\n",
        "        idx  = self.pointer % self.buffer_size \n",
        "        self.state_mem[idx] = state\n",
        "        self.action_mem[idx] = action\n",
        "        self.reward_mem[idx] = reward\n",
        "        self.next_state_mem[idx] = next_state\n",
        "        self.done_mem[idx] = 1 - int(done)\n",
        "        self.pointer += 1\n",
        "\n",
        "    def sample_exp(self, batch_size= 64):\n",
        "        max_mem = min(self.pointer, self.buffer_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "        states = self.state_mem[batch]\n",
        "        actions = self.action_mem[batch]\n",
        "        rewards = self.reward_mem[batch]\n",
        "        next_states = self.next_state_mem[batch]\n",
        "        dones = self.done_mem[batch]\n",
        "        return states, actions, rewards, next_states, dones\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIo_4GpjDukC",
        "cell_id": "00006-cbf92a6c-3abf-4841-8f8d-28d772751097",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "aee29a99",
        "execution_millis": 41,
        "execution_start": 1618216441681,
        "deepnote_cell_type": "code"
      },
      "source": [
        "class agent():\n",
        "      def __init__(self, gamma=0.99, replace=100, lr=0.001):\n",
        "          self.gamma = gamma\n",
        "          self.epsilon = 1.0\n",
        "          self.min_epsilon = 0.01\n",
        "          self.epsilon_decay = 1e-3\n",
        "          self.replace = replace\n",
        "          self.trainstep = 0\n",
        "          self.memory = exp_replay()\n",
        "          self.batch_size = 64\n",
        "          self.q_net = DDDQN()\n",
        "          self.target_net = DDDQN()\n",
        "          opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "          self.q_net.compile(loss='mse', optimizer=opt)\n",
        "          self.target_net.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "\n",
        "      def act(self, state):\n",
        "          if np.random.rand() <= self.epsilon:\n",
        "              return np.random.choice([i for i in range(5)]) # TODO: change the 5\n",
        "\n",
        "          else:\n",
        "              actions = self.q_net.advantage(np.array([state]))\n",
        "              action = np.argmax(actions)\n",
        "              return action\n",
        "\n",
        "      def update_mem(self, state, action, reward, next_state, done):\n",
        "          self.memory.add_exp(state, action, reward, next_state, done)\n",
        "\n",
        "      def update_target(self):\n",
        "          self.target_net.set_weights(self.q_net.get_weights())     \n",
        "\n",
        "      def update_epsilon(self):\n",
        "          self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
        "          return self.epsilon\n",
        "   \n",
        "      def train(self):\n",
        "          if self.memory.pointer < self.batch_size:\n",
        "             return \n",
        "          \n",
        "          if self.trainstep % self.replace == 0:\n",
        "             self.update_target()\n",
        "          states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
        "          target = self.q_net.predict(states)\n",
        "          next_state_val = self.target_net.predict(next_states)\n",
        "          max_action = np.argmax(self.q_net.predict(next_states), axis=1)\n",
        "          batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "          q_target = np.copy(target)\n",
        "          q_target[batch_index, actions] = rewards + self.gamma * next_state_val[batch_index, max_action]*dones\n",
        "          self.q_net.train_on_batch(states, q_target)\n",
        "          self.update_epsilon()\n",
        "          self.trainstep += 1\n",
        "\n",
        "      def save_model(self):\n",
        "          self.q_net.save(\"model.h5\")\n",
        "          self.target_net.save(\"target_model.h5\")\n",
        "\n",
        "      def load_model(self):\n",
        "          self.q_net = load_model(\"model.h5\")\n",
        "          self.target_net = load_model(\"model.h5\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twbsquz5S2vE"
      },
      "source": [
        "agent007 = agent()\n",
        "agent007.load_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-367ebf90-92ca-4d7d-ac12-7c409e317dbd",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8f52a5e8",
        "execution_millis": 28306,
        "execution_start": 1618216475005,
        "deepnote_cell_type": "code",
        "id": "-7ZuE9iNOwHX",
        "outputId": "62ba29d8-7a26-49ee-901b-0b5703dec904",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "saving_interval = 50\n",
        "# Train for 300 episodes\n",
        "for episode in range(3000):\n",
        "    # Initialize episode\n",
        "    states, info = env.reset()\n",
        "    #env_renderer = RenderTool(env)\n",
        "    done = {i: False for i in range(0, num_agents)}\n",
        "    done[\"__all__\"] = False\n",
        "    scores = 0\n",
        "    scores_window = []\n",
        "\n",
        "    \n",
        "    while not done[\"__all__\"]:\n",
        "        actions = {}\n",
        "        agents_obs = {}\n",
        "        \n",
        "        for i in range(0, num_agents):\n",
        "            if not done[i]:\n",
        "                agents_obs[i] = normalize_observation(states[i], tree_depth, radius_observation)\n",
        "                actions[i] = agent007.act(agents_obs[i])\n",
        "            \n",
        "                \n",
        "        next_obs, all_rewards, done, info = env.step(actions) #base env\n",
        "        #env_renderer.render_env(show=True)\n",
        "        for i in range(num_agents):\n",
        "            if not done[i]:\n",
        "                normalized_next_obs = normalize_observation(next_obs[i], tree_depth, radius_observation)\n",
        "                agent007.update_mem(agents_obs[i], actions[i], all_rewards[i], normalized_next_obs, done[i])\n",
        "            elif i in agents_obs: #SIMMY: It was just an else. It worked on the first loop after the \"done\", but then we set agents_obs to {} so it wasn't filled anymore!\n",
        "                agent007.update_mem(agents_obs[i], actions[i], all_rewards[i], agents_obs[i], done[i])\n",
        "            scores += all_rewards[i]/num_agents\n",
        "        agent007.train()\n",
        "        scores_window.append(scores)\n",
        "    print(f\"Final reward in episode {episode} was {np.mean(scores_window)}\")\n",
        "    if(episode%saving_interval == 0):\n",
        "      agent.save_model()\n",
        "    #sum_rewards += reward\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final reward in episode 0 was -47.11666666666667\n",
            "Final reward in episode 1 was -49.666666666666664\n",
            "Final reward in episode 2 was -50.49\n",
            "Final reward in episode 3 was -35.207070707070706\n",
            "Final reward in episode 4 was -25.721153846153847\n",
            "Final reward in episode 5 was -75.5\n",
            "Final reward in episode 6 was -75.5\n",
            "Final reward in episode 7 was -75.5\n",
            "Final reward in episode 8 was -75.5\n",
            "Final reward in episode 9 was -75.5\n",
            "Final reward in episode 10 was -74.65666666666667\n",
            "Final reward in episode 11 was -44.9\n",
            "Final reward in episode 12 was -75.5\n",
            "Final reward in episode 13 was -60.3\n",
            "Final reward in episode 14 was -75.5\n",
            "Final reward in episode 15 was -27.967741935483872\n",
            "Final reward in episode 16 was -42.13\n",
            "Final reward in episode 17 was -75.5\n",
            "Final reward in episode 18 was -65.23\n",
            "Final reward in episode 19 was -75.5\n",
            "Final reward in episode 20 was -75.5\n",
            "Final reward in episode 21 was -50.49\n",
            "Final reward in episode 22 was -75.5\n",
            "Final reward in episode 23 was -37.75\n",
            "Final reward in episode 24 was -41.404761904761905\n",
            "Final reward in episode 25 was -75.5\n",
            "Final reward in episode 26 was -75.5\n",
            "Final reward in episode 27 was -58.666666666666664\n",
            "Final reward in episode 28 was -75.5\n",
            "Final reward in episode 29 was -75.5\n",
            "Final reward in episode 30 was -7.285714285714286\n",
            "Final reward in episode 31 was -75.5\n",
            "Final reward in episode 32 was -50.49\n",
            "Final reward in episode 33 was -8.775\n",
            "Final reward in episode 34 was -43.53\n",
            "Final reward in episode 35 was -75.5\n",
            "Final reward in episode 36 was -43.99\n",
            "Final reward in episode 37 was -54.78\n",
            "Final reward in episode 38 was -75.5\n",
            "Final reward in episode 39 was -75.5\n",
            "Final reward in episode 40 was -38.25\n",
            "Final reward in episode 41 was -63.03\n",
            "Final reward in episode 42 was -75.5\n",
            "Final reward in episode 43 was -38.74666666666667\n",
            "Final reward in episode 44 was -50.89666666666667\n",
            "Final reward in episode 45 was -54.78\n",
            "Final reward in episode 46 was -75.5\n",
            "Final reward in episode 47 was -75.5\n",
            "Final reward in episode 48 was -14.957142857142857\n",
            "Final reward in episode 49 was -7.28125\n",
            "Final reward in episode 50 was -66.74\n",
            "Final reward in episode 51 was -75.5\n",
            "Final reward in episode 52 was -49.25\n",
            "Final reward in episode 53 was -61.24\n",
            "Final reward in episode 54 was -75.5\n",
            "Final reward in episode 55 was -75.5\n",
            "Final reward in episode 56 was -75.5\n",
            "Final reward in episode 57 was -40.21666666666667\n",
            "Final reward in episode 58 was -75.5\n",
            "Final reward in episode 59 was -53.65\n",
            "Final reward in episode 60 was -75.5\n",
            "Final reward in episode 61 was -75.5\n",
            "Final reward in episode 62 was -39.73\n",
            "Final reward in episode 63 was -75.5\n",
            "Final reward in episode 64 was -68.78\n",
            "Final reward in episode 65 was -75.5\n",
            "Final reward in episode 66 was -43.53\n",
            "Final reward in episode 67 was -75.5\n",
            "Final reward in episode 68 was -41.18\n",
            "Final reward in episode 69 was -75.5\n",
            "Final reward in episode 70 was -42.6\n",
            "Final reward in episode 71 was -73.15666666666667\n",
            "Final reward in episode 72 was -52.49\n",
            "Final reward in episode 73 was -75.5\n",
            "Final reward in episode 74 was -51.7\n",
            "Final reward in episode 75 was -75.5\n",
            "Final reward in episode 76 was -60.3\n",
            "Final reward in episode 77 was -49.25\n",
            "Final reward in episode 78 was -48.83\n",
            "Final reward in episode 79 was -41.18\n",
            "Final reward in episode 80 was -75.5\n",
            "Final reward in episode 81 was -60.61666666666667\n",
            "Final reward in episode 82 was -7.113636363636363\n",
            "Final reward in episode 83 was -15.976190476190476\n",
            "Final reward in episode 84 was -46.265517241379314\n",
            "Final reward in episode 85 was -5.681818181818182\n",
            "Final reward in episode 86 was -75.5\n",
            "Final reward in episode 87 was -75.5\n",
            "Final reward in episode 88 was -75.5\n",
            "Final reward in episode 89 was -50.89666666666667\n",
            "Final reward in episode 90 was -75.5\n",
            "Final reward in episode 91 was -9.205882352941176\n",
            "Final reward in episode 92 was -10.189655172413794\n",
            "Final reward in episode 93 was -75.5\n",
            "Final reward in episode 94 was -75.5\n",
            "Final reward in episode 95 was -12.576923076923077\n",
            "Final reward in episode 96 was -6.34375\n",
            "Final reward in episode 97 was -40.21666666666667\n",
            "Final reward in episode 98 was -75.5\n",
            "Final reward in episode 99 was -75.5\n",
            "Final reward in episode 100 was -58.666666666666664\n",
            "Final reward in episode 101 was -57.64666666666667\n",
            "Final reward in episode 102 was -53.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "qFcbQT__OwHX"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c8b2a743-4403-48d9-b1f8-a1215902878c' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ]
}